{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ocsvm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4995909016527573\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.49      0.66      6000\n",
      "         1.0       0.03      0.91      0.06       111\n",
      "\n",
      "    accuracy                           0.50      6111\n",
      "   macro avg       0.51      0.70      0.36      6111\n",
      "weighted avg       0.98      0.50      0.65      6111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 載入數據\n",
    "file_path = '/Users/linyinghsiao/Desktop/chatgpt_output拷貝.csv'  # 替換為您的文件路徑\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 處理缺失值\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "# 編碼類別型變量\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X = data.drop('label', axis=1)\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 分割數據為訓練集和測試集\n",
    "normal_data = data[data['label'] == 0]\n",
    "anomalous_data = data[data['label'] == 1]\n",
    "\n",
    "train_normal = normal_data.sample(frac=0.8, random_state=42)\n",
    "test_normal = normal_data.drop(train_normal.index)\n",
    "test_data = pd.concat([test_normal, anomalous_data], axis=0)\n",
    "\n",
    "X_train = scaler.transform(train_normal.drop('label', axis=1))\n",
    "X_test = scaler.transform(test_data.drop('label', axis=1))\n",
    "y_test = test_data['label']\n",
    "\n",
    "# 訓練 One-Class SVM 模型\n",
    "ocsvm_model = OneClassSVM(kernel='rbf', gamma='auto')\n",
    "ocsvm_model.fit(X_train)\n",
    "\n",
    "# 在測試集上進行預測\n",
    "y_pred_test = ocsvm_model.predict(X_test)\n",
    "y_pred_test = (y_pred_test == -1).astype(int)\n",
    "\n",
    "# 評估模型\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_report = classification_report(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy: {test_accuracy}\")\n",
    "print(f\"Classification Report: \\n{test_report}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9131878715419615\n",
      "F1 Score: 0.04598540145985402\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 載入數據\n",
    "file_path = '/Users/linyinghsiao/Desktop/chatgpt_output拷貝.csv'  # 替換為您的文件路徑\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 處理缺失值\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "# 編碼類別型變量\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# 標準化特徵\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(data.drop('label', axis=1))\n",
    "\n",
    "# 創建並訓練 Isolation Forest 模型\n",
    "iso_forest_model = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "iso_forest_model.fit(X_scaled)\n",
    "\n",
    "# 在數據集上進行預測\n",
    "y_pred_iso = iso_forest_model.predict(X_scaled)\n",
    "y_pred_iso = (y_pred_iso == -1).astype(int)\n",
    "\n",
    "# 評估模型\n",
    "iso_forest_accuracy = accuracy_score(data['label'], y_pred_iso)\n",
    "iso_forest_f1 = f1_score(data['label'], y_pred_iso)\n",
    "\n",
    "print(f\"Accuracy: {iso_forest_accuracy}\")\n",
    "print(f\"F1 Score: {iso_forest_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "753/753 [==============================] - 1s 749us/step - loss: 0.8026 - val_loss: 0.5903\n",
      "Epoch 2/50\n",
      "753/753 [==============================] - 0s 570us/step - loss: 0.5783 - val_loss: 0.5053\n",
      "Epoch 3/50\n",
      "753/753 [==============================] - 0s 615us/step - loss: 0.5029 - val_loss: 0.4466\n",
      "Epoch 4/50\n",
      "753/753 [==============================] - 0s 588us/step - loss: 0.4602 - val_loss: 0.4205\n",
      "Epoch 5/50\n",
      "753/753 [==============================] - 0s 620us/step - loss: 0.4341 - val_loss: 0.3894\n",
      "Epoch 6/50\n",
      "753/753 [==============================] - 0s 603us/step - loss: 0.4115 - val_loss: 0.3682\n",
      "Epoch 7/50\n",
      "753/753 [==============================] - 0s 585us/step - loss: 0.3930 - val_loss: 0.3504\n",
      "Epoch 8/50\n",
      "753/753 [==============================] - 0s 651us/step - loss: 0.3807 - val_loss: 0.3438\n",
      "Epoch 9/50\n",
      "753/753 [==============================] - 0s 602us/step - loss: 0.3656 - val_loss: 0.3190\n",
      "Epoch 10/50\n",
      "753/753 [==============================] - 0s 594us/step - loss: 0.3526 - val_loss: 0.3099\n",
      "Epoch 11/50\n",
      "753/753 [==============================] - 1s 716us/step - loss: 0.3427 - val_loss: 0.3075\n",
      "Epoch 12/50\n",
      "753/753 [==============================] - 0s 656us/step - loss: 0.3358 - val_loss: 0.2932\n",
      "Epoch 13/50\n",
      "753/753 [==============================] - 0s 600us/step - loss: 0.3324 - val_loss: 0.2898\n",
      "Epoch 14/50\n",
      "753/753 [==============================] - 0s 595us/step - loss: 0.3274 - val_loss: 0.2897\n",
      "Epoch 15/50\n",
      "753/753 [==============================] - 0s 569us/step - loss: 0.3239 - val_loss: 0.2841\n",
      "Epoch 16/50\n",
      "753/753 [==============================] - 0s 573us/step - loss: 0.3225 - val_loss: 0.2837\n",
      "Epoch 17/50\n",
      "753/753 [==============================] - 0s 566us/step - loss: 0.3196 - val_loss: 0.2833\n",
      "Epoch 18/50\n",
      "753/753 [==============================] - 0s 610us/step - loss: 0.3170 - val_loss: 0.2800\n",
      "Epoch 19/50\n",
      "753/753 [==============================] - 0s 562us/step - loss: 0.3133 - val_loss: 0.2894\n",
      "Epoch 20/50\n",
      "753/753 [==============================] - 0s 589us/step - loss: 0.3121 - val_loss: 0.2763\n",
      "Epoch 21/50\n",
      "753/753 [==============================] - 0s 607us/step - loss: 0.3089 - val_loss: 0.2758\n",
      "Epoch 22/50\n",
      "753/753 [==============================] - 0s 582us/step - loss: 0.3053 - val_loss: 0.2819\n",
      "Epoch 23/50\n",
      "753/753 [==============================] - 0s 648us/step - loss: 0.3036 - val_loss: 0.2777\n",
      "Epoch 24/50\n",
      "753/753 [==============================] - 0s 645us/step - loss: 0.2977 - val_loss: 0.3006\n",
      "Epoch 25/50\n",
      "753/753 [==============================] - 1s 802us/step - loss: 0.2973 - val_loss: 0.2766\n",
      "Epoch 26/50\n",
      "753/753 [==============================] - 1s 695us/step - loss: 0.2928 - val_loss: 0.2740\n",
      "Epoch 27/50\n",
      "753/753 [==============================] - 1s 705us/step - loss: 0.2924 - val_loss: 0.2754\n",
      "Epoch 28/50\n",
      "753/753 [==============================] - 0s 617us/step - loss: 0.2905 - val_loss: 0.2735\n",
      "Epoch 29/50\n",
      "753/753 [==============================] - 0s 628us/step - loss: 0.2869 - val_loss: 0.2785\n",
      "Epoch 30/50\n",
      "753/753 [==============================] - 1s 682us/step - loss: 0.2873 - val_loss: 0.2763\n",
      "Epoch 31/50\n",
      "753/753 [==============================] - 0s 641us/step - loss: 0.2851 - val_loss: 0.2711\n",
      "Epoch 32/50\n",
      "753/753 [==============================] - 0s 629us/step - loss: 0.2820 - val_loss: 0.2716\n",
      "Epoch 33/50\n",
      "753/753 [==============================] - 0s 628us/step - loss: 0.2809 - val_loss: 0.2724\n",
      "Epoch 34/50\n",
      "753/753 [==============================] - 0s 629us/step - loss: 0.2793 - val_loss: 0.2788\n",
      "Epoch 35/50\n",
      "753/753 [==============================] - 0s 597us/step - loss: 0.2806 - val_loss: 0.2717\n",
      "Epoch 36/50\n",
      "753/753 [==============================] - 0s 543us/step - loss: 0.2776 - val_loss: 0.2730\n",
      "Epoch 37/50\n",
      "753/753 [==============================] - 0s 610us/step - loss: 0.2775 - val_loss: 0.2728\n",
      "Epoch 38/50\n",
      "753/753 [==============================] - 0s 543us/step - loss: 0.2764 - val_loss: 0.2886\n",
      "Epoch 39/50\n",
      "753/753 [==============================] - 0s 547us/step - loss: 0.2758 - val_loss: 0.2749\n",
      "Epoch 40/50\n",
      "753/753 [==============================] - 0s 544us/step - loss: 0.2745 - val_loss: 0.2779\n",
      "Epoch 41/50\n",
      "753/753 [==============================] - 0s 538us/step - loss: 0.2735 - val_loss: 0.2735\n",
      "Epoch 42/50\n",
      "753/753 [==============================] - 0s 545us/step - loss: 0.2740 - val_loss: 0.2829\n",
      "Epoch 43/50\n",
      "753/753 [==============================] - 0s 547us/step - loss: 0.2731 - val_loss: 0.2811\n",
      "Epoch 44/50\n",
      "753/753 [==============================] - 0s 543us/step - loss: 0.2719 - val_loss: 0.2710\n",
      "Epoch 45/50\n",
      "753/753 [==============================] - 0s 539us/step - loss: 0.2717 - val_loss: 0.2697\n",
      "Epoch 46/50\n",
      "753/753 [==============================] - 0s 573us/step - loss: 0.2735 - val_loss: 0.2690\n",
      "Epoch 47/50\n",
      "753/753 [==============================] - 0s 601us/step - loss: 0.2711 - val_loss: 0.2699\n",
      "Epoch 48/50\n",
      "753/753 [==============================] - 0s 556us/step - loss: 0.2698 - val_loss: 0.2707\n",
      "Epoch 49/50\n",
      "753/753 [==============================] - 0s 547us/step - loss: 0.2709 - val_loss: 0.2754\n",
      "Epoch 50/50\n",
      "753/753 [==============================] - 0s 542us/step - loss: 0.2712 - val_loss: 0.2715\n",
      "941/941 [==============================] - 0s 284us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "# 載入數據\n",
    "file_path = '/Users/linyinghsiao/Desktop/chatgpt_output拷貝.csv'  # 替換為您的文件路徑\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 處理缺失值\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# 處理非數值型列\n",
    "label_encoder = LabelEncoder()\n",
    "non_numeric_columns = data.select_dtypes(include=['object']).columns\n",
    "for col in non_numeric_columns:\n",
    "    data[col] = label_encoder.fit_transform(data[col])\n",
    "\n",
    "# 標準化特徵\n",
    "X_scaled = StandardScaler().fit_transform(data.drop('label', axis=1))\n",
    "\n",
    "# 創建 Autoencoder 模型\n",
    "input_dim = X_scaled.shape[1]\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(16, activation=\"relu\")(input_layer)\n",
    "encoder = Dense(8, activation=\"relu\")(encoder)\n",
    "decoder = Dense(16, activation=\"relu\")(encoder)\n",
    "decoder = Dense(input_dim, activation=\"linear\")(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "# 編譯和訓練模型\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "autoencoder.fit(X_scaled, X_scaled, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# 使用模型對數據進行預測，並計算重建誤差\n",
    "predictions = autoencoder.predict(X_scaled)\n",
    "mse = np.mean(np.power(X_scaled - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse, 'true_class': data['label']})\n",
    "\n",
    "# 根據重建誤差識別異常\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034607</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.120706</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.088149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>0.581031</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>0.240890</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30108</th>\n",
       "      <td>0.753817</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30109</th>\n",
       "      <td>0.108775</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30110</th>\n",
       "      <td>0.232086</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30111 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error  true_class\n",
       "0                  0.034607         0.0\n",
       "1                  0.120706         0.0\n",
       "2                  0.088149         0.0\n",
       "3                  0.095376         0.0\n",
       "4                  0.130722         0.0\n",
       "...                     ...         ...\n",
       "30106              0.581031         1.0\n",
       "30107              0.240890         1.0\n",
       "30108              0.753817         1.0\n",
       "30109              0.108775         1.0\n",
       "30110              0.232086         1.0\n",
       "\n",
       "[30111 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df.to_csv('../datasets/dataset_1st/result_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   reconstruction_error  true_class\n",
       " 0              0.034607         0.0\n",
       " 1              0.120706         0.0\n",
       " 2              0.088149         0.0\n",
       " 3              0.095376         0.0\n",
       " 4              0.130722         0.0,\n",
       " count    30111.000000\n",
       " mean         0.268218\n",
       " std          1.680325\n",
       " min          0.011050\n",
       " 25%          0.099620\n",
       " 50%          0.161950\n",
       " 75%          0.265251\n",
       " max        265.903235\n",
       " Name: reconstruction_error, dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display the first few rows of the dataframe and summary statistics\n",
    "error_df_head = error_df.head()\n",
    "error_df_description = error_df['reconstruction_error'].describe()\n",
    "\n",
    "error_df_head, error_df_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.628867371909841, 250, 29861)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the threshold based on mean and standard deviation\n",
    "#根據所選的閾值策略（平均重建誤差加上兩倍標準差），我們確定了以下結果：\n",
    "#計算出的閾值為約 3.63。\n",
    "#根據此閾值，被識別為異常的樣本數量為 250。\n",
    "#被識別為正常的樣本數量為 29,861。\n",
    "mean_error = error_df_description['mean']\n",
    "std_error = error_df_description['std']\n",
    "threshold = mean_error + 2 * std_error\n",
    "\n",
    "# Identifying anomalies\n",
    "error_df['anomaly'] = error_df['reconstruction_error'] > threshold\n",
    "\n",
    "# Count of anomalies and normal data points\n",
    "anomaly_count = error_df['anomaly'].sum()\n",
    "normal_count = len(error_df) - anomaly_count\n",
    "\n",
    "threshold, anomaly_count, normal_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9884759722360599,\n",
       " 0.028,\n",
       " 0.06306306306306306,\n",
       " 0.038781163434903045,\n",
       " array([[29757,   243],\n",
       "        [  104,     7]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Extracting the true labels and predicted anomaly labels\n",
    "y_true = error_df['true_class']\n",
    "y_pred_anomaly = error_df['anomaly']\n",
    "\n",
    "# Calculating evaluation metrics\n",
    "accuracy = accuracy_score(y_true, y_pred_anomaly)\n",
    "precision = precision_score(y_true, y_pred_anomaly)\n",
    "recall = recall_score(y_true, y_pred_anomaly)\n",
    "f1 = f1_score(y_true, y_pred_anomaly)\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_anomaly)\n",
    "\n",
    "accuracy, precision, recall, f1, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
